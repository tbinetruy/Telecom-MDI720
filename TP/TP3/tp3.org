#+TITLE: Stats TP3
#+HTML_HEAD: <style>html { max-width: 50rem; margin: auto }</style>
#+HTML_HEAD: <style>.figure img { width: 100% }</style>
#+HTML_HEAD: <style>pre.example { background-color: aliceblue }</style>


#+begin_src bash :exports none
  mkdir -p img
  ls | grep img
#+end_src

* Introduction
  
Nous travaillons sur la base de données ~diabetes~ de python. La base initiale comporte $n = 442$ patients et $p = 10$ covariables. La variable $Y$ à expliquer est un score correspondant à l’évolution de la maladie. Pour s’amuser, un robot malicieux a contaminé le jeu de données en y ajoutant 200 variables explicatives inappropriées. Ensuite, non-content d’avoir déjà perverti notre jeu de données, il a volontairement mélangé les variables entre elle de façon aléatoire. Bien entendu le robot a ensuite pris soin d’effacer toute trace de son acte crapuleux si bien que nous ne connaissons pas les variables pertinentes. La nouvelle base de données comporte $n = 442$ patients et $p = 210$ covariables, notés $X$. Saurez-vous déjouer les plans de ce robot farceur et retrouver les variables pertinentes ?

* OLS Library
  
#+BEGIN_SRC python :session default :exports both :results output :tangle yes
def get_X(X_tild):
  X_tild = X_tild.tolist()
  X = [[1] +  X_tild[i] for i in range(0, len(X_tild))]
  return np.matrix(X)

# X [numpy matrix] is an n by p + 1 matrix
# Y [numpy matrix] is a column vector of n elements
# returns the n-vector for our estimator
def get_theta_hat(X, Y): 
    G = inv(X.T@X)
    theta_hat = G@X.T@Y
    
    return theta_hat

# Y [numpy matrix] - vector to calculate arithmetic mean against
# Returns the arithmetic mean of Y
def get_mean(Y):
    return sum(Y) / len(Y)

# x [numpy matrix] - the point to find an estimation for, a row vector of length n
# theta_hat [numpy matrix] - our estimator column vector of n elements
# Returns the estimation at point x given the estimator theta_hat
def get_Y_hat(X, theta_hat):
    return X@theta_hat

# Calculates the determination coefficient for our X matrix and Y vector
def get_R2(X_train, Y_train, X_test, Y_test):
    X_train = get_X(X_train)
    X_train = np.matrix(X_train)
    X_test = get_X(X_test)
    X_test = np.matrix(X_test)

    theta_hat = get_theta_hat(X_train, Y_train)
    Y_bar = get_Y_bar(Y_test)
    Y_hat = get_Y_hat(X_test, theta_hat)
    SSR = sum([(Y_hat[i] - Y_test[i])**2 for i in range(0, len(Y_test))])
    SST = sum([(Y_test[i] - Y_bar)**2 for i in range(0, len(Y_test))])
    return 1 - SSR / SST

# Noise variance for X and Y
def get_sig2(X, Y):
    theta_hat = get_theta_hat(X, Y)
    Y_hat = get_Y_hat(X, theta_hat)
    return sum([(Y_hat[i] - Y[i])**2 for i in range(0, len(Y))]) / (len(Y) - 2)

# Covariance matrix for X and Y
def get_estimator_covariance_mat(X, Y):
    sig2 = get_sig2(X, Y).item()
    return sig2 * inv(X.T@X)

# Helper pour calculer le rang d'une matrix X via Numpy
def rank(X):
    return np.linalg.matrix_rank(X)

# Calculates the t-test for the i'th estimator of our X-Y linear system
def student_test_i(X, Y, i):
    theta_hat = get_theta_hat(X, Y)
    var_thetaHat = get_estimator_covariance_mat(X, Y)
    return abs(theta_hat[i] / math.sqrt(var_thetaHat[i,i]))

# Returns a vector containing all the t-tests for our estimators (in the same order)
def student_test(X, Y):
    return [student_test_i(X, Y, i) for i in range(0, rank(X))]

# Helper to get Student distribution of degree deg an quantile of level lev
def get_students_quantile(deg, level):
    return scipy.stats.t(df=deg).ppf(level)

#+END_SRC

#+RESULTS:


* Imports
  
#+BEGIN_SRC python :exports both :session default :tangle yes
import math
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy import stat
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.linear_model import Ridge, LassoCV, LinearRegression
from numpy.linalg import inv
#+END_SRC

#+RESULTS:

* Chargement des données

#+BEGIN_SRC python :session default :exports both :tangle yes
def import_data():
    df = pd.read_csv('data_dm3.csv', delimiter=",", header=None)
    return df

df = import_data()
df.head()
#+END_SRC

#+RESULTS:
:         0         1         2         3         4         5    ...         205       206       207       208       209    210
: 0 -1.298173 -0.162249  1.223379  1.355554  1.080171  0.634979  ...    0.279299 -1.416020 -2.332363  0.215096 -0.693319  151.0
: 1  0.166951 -0.338060 -0.618867  0.759366  1.134281 -0.536844  ...    0.225958 -0.822288  0.382838 -0.718829 -0.188993   75.0
: 2 -0.416177 -0.205659 -1.282226  1.675500  1.523746  0.192029  ...    0.139267 -1.901196  0.048210  0.220205  0.471588  141.0
: 3  0.867184 -0.398667  0.093501  0.025971  1.852099  0.789774  ...    0.723819  1.316367  0.088218  0.619496  1.061662  206.0
: 4  1.193282 -0.936980 -0.725039  0.766078  0.223489 -1.584622  ...    2.040010 -1.703110 -1.901502  1.778811 -0.489853  135.0
: 
: [5 rows x 211 columns]


Séparons maintenant le variable à expliquer, c'est a dire la dernière colonne de notre dataframe. Les variables ~dfX~ et ~dfY~ seront les dataframes contenant la matrice de design et la variables à expliquer:

#+BEGIN_SRC python :session default : exports both :tangle yes
num_df_cols = df.shape[1] - 1
dfX = df.drop(num_df_cols, axis=1)
dfX.head()
#+END_SRC

#+RESULTS:
:         0         1         2         3         4         5      ...          204       205       206       207       208       209
: 0 -1.298173 -0.162249  1.223379  1.355554  1.080171  0.634979    ...    -0.436399  0.279299 -1.416020 -2.332363  0.215096 -0.693319
: 1  0.166951 -0.338060 -0.618867  0.759366  1.134281 -0.536844    ...     1.119430  0.225958 -0.822288  0.382838 -0.718829 -0.188993
: 2 -0.416177 -0.205659 -1.282226  1.675500  1.523746  0.192029    ...    -2.579347  0.139267 -1.901196  0.048210  0.220205  0.471588
: 3  0.867184 -0.398667  0.093501  0.025971  1.852099  0.789774    ...    -0.884172  0.723819  1.316367  0.088218  0.619496  1.061662
: 4  1.193282 -0.936980 -0.725039  0.766078  0.223489 -1.584622    ...    -0.642504  2.040010 -1.703110 -1.901502  1.778811 -0.489853
: 
: [5 rows x 210 columns]

#+BEGIN_SRC python :session default :exports both :tangle yes
dfY = df[num_df_cols]
dfY.head()
#+END_SRC

#+RESULTS:
: 0    151.0
: 1     75.0
: 2    141.0
: 3    206.0
: 4    135.0
: Name: 210, dtype: float64

* Question 1
  
/Importer la base de données ~data_dm3.csv~ disponible depuis le lien https://bitbucket.org/portierf/shared_files/downloads/data_dm3.csv. La dernière colonne est la variable à expliquer. Les autres colonnes sont les variables explicatives. Préciser le nombre de variables explicatives et le nombre d’observations./


#+BEGIN_SRC python :session default :results output :exports both :tangle yes
print("Nombre de variable explicatives:", dfX.shape[1])
print("Numbre d'observations", dfX.shape[0])
#+END_SRC

#+RESULTS:
: Nombre de variable explicatives: 210
: Numbre d'observations 442

* Question 2
  
/Les variables explicatives sont-elles centrées ? Normalisées ? Qu’en est-il de la variable à expliquer ? Tracer un scatter plot de la base de données avec 4 covariables prises au hasard et la variable à expliquer (un scatterplot regroupe les graphes de chacune des variables en fonction de chacune des autres). Commenter les graphiques obtenus./

Analysons dans un premier temps les moyennes et variances des colonnes de ~dfX~ et ~dfY~:

#+BEGIN_SRC python :session default :exports both :tangle yes
dfX.mean(axis=0)
#+END_SRC

#+RESULTS:
#+begin_example
0      7.535450e-19
1     -1.507090e-17
2      5.494599e-20
3     -7.284269e-18
4      8.288995e-18
5     -2.712762e-17
6      1.971776e-17
7      8.540177e-18
8      1.029845e-17
9      4.018907e-18
10    -1.444295e-17
11     3.717489e-17
12    -3.642134e-17
13    -1.124038e-17
14    -3.750456e-17
15    -4.511851e-17
16     2.461580e-17
17     9.293722e-18
18     2.662526e-17
19    -5.601351e-17
20    -3.067556e-17
21    -4.521270e-18
22     6.781905e-18
23    -4.056584e-17
24     1.004727e-18
25    -2.813235e-17
26    -3.540092e-17
27    -5.953006e-17
28    -4.533829e-17
29     3.064416e-17
           ...     
180   -3.767725e-18
181    3.843080e-17
182    4.018907e-18
183    6.380015e-17
184    1.795949e-17
185   -1.306145e-17
186    1.550053e-17
187    3.918434e-17
188    1.871304e-17
189    1.356381e-17
190   -2.737880e-17
191    2.210399e-17
192   -3.843080e-17
193    4.511851e-17
194   -6.530724e-18
195   -3.014180e-17
196    3.014180e-17
197   -2.888589e-17
198    7.887105e-17
199    3.918434e-17
200    1.934099e-17
201   -2.260635e-18
202   -2.637408e-17
203   -5.023634e-19
204   -1.538488e-17
205    5.525997e-18
206    3.265362e-17
207    1.507090e-17
208   -4.034606e-18
209    1.205672e-17
Length: 210, dtype: float64
#+end_example

#+BEGIN_SRC python :session default :exports both :tangle yes
dfX.var(axis=0)
#+END_SRC

#+RESULTS:
#+begin_example
0      1.002268
1      1.002268
2      1.002268
3      1.002268
4      1.002268
5      1.002268
6      1.002268
7      1.002268
8      1.002268
9      1.002268
10     1.002268
11     1.002268
12     1.002268
13     1.002268
14     1.002268
15     1.002268
16     1.002268
17     1.002268
18     1.002268
19     1.002268
20     1.002268
21     1.002268
22     1.002268
23     1.002268
24     1.002268
25     1.002268
26     1.002268
27     1.002268
28     1.002268
29     1.002268
         ...   
180    1.002268
181    1.002268
182    1.002268
183    1.002268
184    1.002268
185    1.002268
186    1.002268
187    1.002268
188    1.002268
189    1.002268
190    1.002268
191    1.002268
192    1.002268
193    1.002268
194    1.002268
195    1.002268
196    1.002268
197    1.002268
198    1.002268
199    1.002268
200    1.002268
201    1.002268
202    1.002268
203    1.002268
204    1.002268
205    1.002268
206    1.002268
207    1.002268
208    1.002268
209    1.002268
Length: 210, dtype: float64
#+end_example

Nous remarquons que les colonnes de la matrice de design sont centrées et de variance proche de 1 à moins du tiers de pourcent près. En effet, la variance est égale à 1.002268 et est constante d'une colonne à l'autre.

Ci-dessous, nous analysons que notre variable explicative n'est ni centrée, ni normalisé:

#+BEGIN_SRC python :session default :exports both :tangle yes :results output
print("La variable explicative à une moyenne de :", dfY.mean(axis=0))
print("La variable explicative à une variance de :", dfY.var(axis=0))
#+END_SRC

#+RESULTS:
: La variable explicative à une moyenne de : 152.13348416289594
: La variable explicative à une variance de : 5943.331347923785

Sélectionnons 4 colonnes au hasard à l'aide de ~random.randint~ et générons une "scatter matrix":

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
# Helper to generate
def rand():
    return random.randint(0, dfX.shape[1] - 1)

rand_cols = [rand() for i in range(4)]

plt.figure(0)
sns_plot = sns.pairplot(dfX[rand_cols])
path2 = "./img/q2.png"
sns_plot.savefig(path2)
plt.close()
path2
#+END_SRC

#+RESULTS:
[[file:./img/q2.png]]

Nous remarquons sur la scatter matrix que les variables sélectionnée pour la réaliser ne sont pas corrélées puisqu'un nuage symétrique de points indique une corrélation de 0 entre les deux axes.

* Question 3
  
/Echantillon d’apprentissage et de test. Créer 2 échantillons : un pour apprendre le modèle $X_{\mbox{train}}$, un pour tester le modèle X test. On mettra 20% de la base dans l’échantillon ’test’. Donner les tailles de chacun des 2 échantillons. On notera que le nouvel échantillon de covariables $X_{\mbox{train}}$ n’est pas normalizé. Dans la suite, on fera donc bien attention à inclure l’intercept dans nos régression./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
X = np.matrix(dfX)
Y = np.matrix(dfY).T

X_test = X[0:88,:]
X_train = X[87:-1,:]

Y_test = Y[0:88]
Y_train = Y[87:-1]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("X test shape:", X_test.shape)
print("X train shape:", X_train.shape)
print("Y test shape:", Y_test.shape)
print("Y train shape:", Y_train.shape)
#+END_SRC

#+RESULTS:
: X test shape: (89, 210)
: X train shape: (353, 210)
: Y test shape: (89, 1)
: Y train shape: (353, 1)


* Question 4
  
/Donner la matrice de covariance calculée sur $X_{\mbox{train}}$. Tracer le graphe de la décroissance des valeurs propres de la matrice de corrélation. Expliquer pourquoi il est légitime de ne garder que les premières variables de l’ACP. On gardera 60 variables dans la suite./

La matrice des correlations est définie de la manière suivante:

$$Cor(X) = (X - \mathbb{E}(X))^T(X - \mathbb{E}(X))$$

Mais nous avons vu dans la question précédente que l'espérance de $X$ est nulle, donc notre matrice des correlations est égale à la matrice de Gram:

$$Cor(X) = X^TX$$.

Nous allons trouver les vecteurs propres de $X^TX$ à l'aide de la SVD. En effet, nous savons que pour une matrice $M \in \mathbb{R}^{n \bigtimes p}$

$$
M = USV^T
$$

Avec une matrice orthogonale $U \in \mathbb{R}^{n \bigtimes n}$, un matrice diagonale $S \in \mathbb{R}^{n \bigtimes p}$, et une matrice orthogonale $V \in \mathbb{R}^{p \bigtimes p}$, donc:

$$
X^TX = (USV^T)^T(USV^T)
= VS^TU^TUSV^T
= VS^TSV^T
$$

Ce qui veut bien dire qu'en prenant la matrice $V$ de la SVD de $X$, nous obtenons les vecteurs propres de notre matrice de corrélation $X^TX$:

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
u, s, vh = np.linalg.svd(X_train)

path3 = "./img/q3.png"
plt.figure(0)
plt.scatter(range(len(s)), s)
plt.savefig(path3)
plt.close()

path3
#+END_SRC

#+RESULTS:
[[file:./img/q3.png]]

Nous voyons bien sur le graphique ci dessus que lorsque nous somme dans la base des vecteurs propres de $X^TX$, seulement 60 des valeurs propres de la matrice de corrélation ne sont pas nulles. C'est à dire que seulement 60 des colonnes de $X$ contiennent des informations affectées par la variations d'autre(s) colonne(s). Les autres colonnes ayant aucune influence sur nos 60 variables, nous pouvons donc les retirer de notre dataframe pour alléger les calculs suivants.

* Question 5
  
/Suivant les observations de la question (Q4), appliquer la méthode de "PCA before OLS" qui consiste à appliquer OLS avec $Y$ et $X_{\mbox{train}}V_{1:60}$, où $V_{1:60}$ contient les vecteurs propres (associé aux 60 plus grandes valeurs propres) de la matrice de covariance. Faire une régression linéaire (avec intercept), puis tracer les valeurs des coefficients (hors intercept). Sur un autre graphique, faire de même avec la méthode des moindres carrés classique./

** PCA before OLS

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
u, s, vh = np.linalg.svd(X_train)
Xpca = X_train@vh.T[:,0:60]
print("La taille de notre matrice de design après PCA est :", Xpca.shape)
theta_hat_pca_before_ols = LinearRegression(fit_intercept=True).fit(Xpca, Y_train)
print("Nos coefficients:")
print(theta_hat_pca_before_ols.coef_)

#+END_SRC

#+RESULTS:
#+begin_example
(353, 60)
[[-1.22330969e+01  2.17054945e+00 -2.03720914e+00 -5.40211814e+00
   2.35586394e+00 -7.76571081e-01  4.05826712e+00 -2.81099700e-01
  -1.77329602e+00 -3.44791089e+00  1.36056670e+00  5.43933482e-01
   1.35972761e+00  5.93832061e-01 -1.23802573e+00  4.24963156e+00
  -1.13911998e+00  5.89940047e-01  7.46534497e-01 -4.30535424e+00
  -6.60808712e-01  2.04583307e-01  1.03349261e-01  4.92049978e+00
   8.65639317e-01  2.38793662e+00  7.19228395e-04  4.24084737e-01
   2.69685873e+00 -2.89653858e+00  1.36145833e+00  2.11745233e+00
   2.49143991e+00 -2.27657513e+00  1.51113345e+00 -2.43503100e+00
  -1.34352655e+00 -2.24311197e+00  3.47332649e+00  3.19838747e+00
   1.19314296e-01 -3.40346223e+00  7.54369729e+00  3.36824649e+00
  -4.41704153e+00  5.12537594e+00 -2.51723512e+00 -6.67013764e+00
  -7.24569531e-01 -7.13199177e+00  7.79086660e+00 -9.72432611e+00
  -1.61102779e+01  2.24268297e+01 -3.06928158e-01  1.11193253e+01
  -7.12279743e+00  4.74128685e-02 -1.25876569e+01 -6.26735917e+01]]
#+end_example


#+BEGIN_SRC python :session default :exports both :results file :tangle yes
plt.figure(0)
path5 = "./q5b.png"
plt.scatter(range(theta_hat_pca_before_ols.coef_.shape[1]), theta_hat_pca_before_ols.coef_)
plt.savefig(path5)
plt.close()
path5
#+END_SRC

#+RESULTS:
[[file:./q5b.png]]

** OLS

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
theta_hat_ols = LinearRegression(fit_intercept=True).fit(X_train, Y_train)
print("OLS intercept", theta_hat_ols.intercept_)
print("X_train shape", X_train.shape)
#+END_SRC

#+RESULTS:
: [153.48863436]
: (354, 210)


#+BEGIN_SRC python :session default :exports both :results file :tangle yes
plt.figure(0)
path4 = "./q5a.png"
plt.scatter(range(theta_hat_ols.coef_.shape[1]), theta_hat_ols.coef_)
plt.savefig(path4)
plt.close()
path4
#+END_SRC

#+RESULTS:
[[file:./q5a.png]]

Nous remarquons quand sans réaliser de PCA avant OLS, nous obtenons des coefficients énormes, de l'ordre de $10^{15}$ pour les plus gros. Alors qu'avec un PCA before OLS, nos plus grands coefficients sont de l'ordre de 20-30.

* Question 6
  
/Donner les valeurs des intercepts pour les 2 régressions précédentes. Donner la valeur moyenne de la variable $Y$ (sur le train set). Les intercepts des 2 questions sont-ils égaux ? Commenter. Uniquement pour cette question, centrer et réduire les variables après ACP (de petite dimension). Faire une régression avec ces variables et vérifier que l’intercept est bien égal à la moyenne de $Y$ sur le train./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
print("Intercept OLS:", theta_hat_ols.intercept_)
print("Intercept PCA before OLS:", theta_hat_pca_before_ols.intercept_)
#+END_SRC

#+RESULTS:
: Intercept OLS: [153.48863436]
: Intercept PCA before OLS: [152.77106978]

Les deux intercepts ne sont donc pas égaux. Nous remarquons que l'intercept avec PCA before OLS est moins proche de la moyenne des $Y_{\mbox{train}}$ que ne l'est l'intercept pour OLS.

Centrons / réduisons maintenant $X_{\mbox{train}}$ et réalisons un PCA before OLS pour comparer la valeur de l'intercept de notre modèle avec la moyenne de $Y_{\mbox{train}}$.

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
  Xpca_c = Xpca-Xpca.mean(axis=0)
  Xpca_n = Xpca_c/np.sqrt(Xpca.var(axis=0))

  q6_model = LinearRegression(fit_intercept=True).fit(Xpca_n, Y_train)
  print("Intercept for OLS after normalizationn of PCA:", q6_model.intercept_)
  print("Y_train mean:", Y_train.mean())
#+END_SRC

#+RESULTS:
: Intercept for OLS after normalizationn of PCA: [156.43785311]
: Y_train mean: 156.43785310734464

Nous trouvons donc la même valeur (à 7 décimales près) entre notre intercept et la moyenne des $Y_{\mbox{train}}$.

* Question 7

/Pour les 2 méthodes (OLS et PCA before OLS) : Tracer les résidus de la prédiction sur l’échantillon test. Tracer leur densité (on pourra par exemple utiliser un histogramme). Calculer le coefficient de détermination sur l’échantillon test. Calculer le risque de prédiction sur l’échantillon test./

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
Y_pred_pca = theta_hat_pca_before_ols.predict(X_test@vh.T[:,:60])
res_pca = Y_test - Y_pred_pca

path7a = "./img/q7a.png"
plt.figure(0)
plt.scatter(range(len(res_pca)), [r[0] for r in res_pca])
plt.title("Résidus PCA before OLS sur l'échantillon test")
plt.ylabel("Résidus")
plt.xlabel("Observations")
plt.savefig(path7a)
plt.close()

path7a
#+END_SRC

#+RESULTS:
[[file:./img/q7a.png]]

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
Y_pred_ols = theta_hat_ols.predict(X_test)
res_ols = Y_test - Y_pred_ols

path7b = "./img/q7b.png"
plt.figure(0)
plt.scatter(range(len(res_ols)), [r[0] for r in res_ols])
plt.title("Résidus OLS sur l'échantillon de test")
plt.ylabel("Résidus")
plt.xlabel("Observations")
plt.savefig(path7b)
plt.close()

path7b
#+END_SRC

#+RESULTS:
[[file:./img/q7b.png]]

Utilisons la librairie Seasborn à nouveau pour réaliser un histogramme de nos résidus pour OLS et PCA before OLS:

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
f, axes = plt.subplots(1, 2)

plt.figure(0)
sns.set()
path7c = "./img/q7c.png"
plt7c = sns.distplot(res_pca, bins=20, kde=True, ax=axes[0])
plt7c = sns.distplot(res_ols, bins=20, kde=True, ax=axes[1])
axes[0].set_title("PCA before OLS")
axes[1].set_title("OLS")
plt7c.figure.savefig(path7c)
plt.close()

path7c
#+END_SRC

#+RESULTS:
[[file:./img/q7c.png]]

Enfin, nous calculons le coefficient de détermination sur l'échantillon de test à l'aide de scikit-learn ainsi que le risque de prédiction normalisée de la manière suivante:

$$
R_{\mbox{pred}} = \frac{||Y_{\mbox{pred}} - Y_{\mbox{test}}||^2}{n}
$$

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
print("R2 for PCA before OLS", theta_hat_pca_before_ols.score(X_test@vh.T[:,:60], Y_test))
print("R2 for OLS", theta_hat_ols.score(X_test, Y_test))

def get_R_pred(Y_test, Y_pred):
  A = Y_test - Y_pred
  return (A.T@A / len(Y_test))[0,0]

R_pred_pca_ols = get_R_pred(Y_test, Y_pred_pca)
R_pred_ols = get_R_pred(Y_test, Y_pred_ols)

print("R_pred PCA before OLS:", R_pred_pca_ols)
print("R_pred OLS:", R_pred_ols)
#+END_SRC

#+RESULTS:
: R2 for PCA before OLS 0.42767779780822845
: R2 for OLS 0.29118125887098567
: R_pred PCA before OLS: 3032.249876257207
: R_pred OLS: 3755.4292526940235

* Question 8

Coder la méthode de forward variable sélection. On pourra utiliser la statistique du test de nullité du coefficient (comme vu en cours). Pour l’instant, on ne met pas de critère d’arrêt sur la méthode. C’est à dire que l’on ajoute une variable à chaque étape jusqu’à retrouver la totalité des variables. Afficher l’ordre de sélection des variables.

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
def get_p_value(t_stat, deg):
    # compute the p-value if t_stat follows a Student of degree deg
    p_val = (1 - stats.t.cdf(abs(t_stat), deg)) * 2
    return p_val

def fast_forward_selection(X_train, Y_train, stop=1000):
  p = X_train.shape[1]
  n = X_train.shape[0]
  res_prev = Y_train
  X = get_X(X_train)
  col_maxs = []
  pvalues = []
  tstats = []
  for i in range(p + 1):
    col_max = 0
    tstat_max = 0
    tstats_j = []

    for j in range(p + 1):
      if j not in col_maxs:
        X_j = X[:, j]
        model_j = LinearRegression(fit_intercept=False).fit(X_j, res_prev)
        X_j_bar = get_mean(X_j)
        Y_pred = model_j.predict(X_j)
        res_new = Y_train - Y_pred
        sigma_est = np.sqrt(1 / (n - 1) * (res_new.T@res_new)[0,0])
        tstat = abs(model_j.coef_[0,0]) / (sigma_est * np.sqrt(inv(X_j.T@X_j)[0,0])) 
        tstats_j.append(tstat)
        if tstat > tstat_max:
          col_max = j
          tstat_max = tstat

    pvalue = get_p_value(tstat_max, n - 1)
    if pvalue < stop:
      res_prev = res_new
      col_maxs.append(col_max)
      pvalues.append(pvalue)
      tstats.append(tstats_j)
    else:
      return [[c - 1 for c in col_maxs[1:]], pvalues, tstats]

  return [[c - 1 for c in col_maxs[1:]], pvalues, tstats]
#+END_SRC

#+RESULTS:

Montrons toutes les variables dans leur ordre de sélection:


#+BEGIN_SRC python :session default :exports both :results output :tangle yes
result = fast_forward_selection(X_train, Y_train)
print("Ordre de selection des variables:", result[0])
print("pvalues", result[1])
#+END_SRC

#+RESULTS:
: Ordre de selection des variables: [58, 123, 167, 23, 34, 174, 99, 94, 7, 133, 103, 27, 115, 112, 17, 92, 121, 128, 162, 148, 197, 146, 79, 33, 110, 22, 45, 18, 19, 60, 182, 168, 43, 84, 86, 207, 158, 160, 44, 177, 53, 85, 81, 4, 138, 49, 159, 95, 11, 143, 188, 13, 116, 50, 136, 129, 183, 88, 147, 55, 142, 76, 29, 20, 178, 131, 208, 3, 47, 62, 9, 108, 195, 153, 35, 106, 149, 28, 72, 179, 16, 166, 75, 100, 169, 102, 5, 150, 161, 97, 191, 90, 175, 105, 61, 185, 109, 119, 114, 78, 74, 209, 65, 120, 186, 24, 172, 41, 91, 2, 77, 93, 176, 104, 130, 82, 152, 134, 59, 46, 139, 83, 66, 187, 101, 157, 87, 189, 141, 140, 192, 117, 193, 127, 54, 113, 51, 164, 171, 165, 36, 71, 107, 200, 180, 202, 68, 198, 89, 31, 14, 155, 201, 184, 80, 137, 132, 69, 64, 70, 42, 48, 154, 56, 145, 111, 206, 124, 32, 122, 126, 125, 37, 199, 63, 190, 39, 135, 15, 38, 204, 73, 25, 12, 118, 170, 196, 156, 163, 98, 30, 194, 6, 21, 144, 1, 96, 40, 57, 26, 8, 151, 0, 67, 203, 205, 173, 10, 181, 52]
: pvalues [0.0, 4.594366220800339e-09, 1.9448257604359753e-07, 2.230973249894852e-05, 6.161408358718035e-05, 0.00014767475703725275, 0.00027147789041803705, 0.028673792423168587, 0.12881412104997314, 0.1278269728079846, 0.13374056459324057, 0.16018636751074244, 0.16447922689648098, 0.18548944477766804, 0.19094514475727165, 0.1970415080138337, 0.22748495556853632, 0.23235013701080254, 0.24447740950053642, 0.23499322582029247, 0.24655907781277797, 0.254934884275261, 0.28562232434025736, 0.2867501573520397, 0.32642615817698895, 0.30640222844631415, 0.3496595391874835, 0.3447585633155388, 0.35480453323896644, 0.35951816254445124, 0.36632892985411325, 0.3678623469738933, 0.3850419138130743, 0.3748753847656594, 0.39088162247592173, 0.4070870417437815, 0.3909556351054544, 0.41281907854523103, 0.4335281985544379, 0.42600433490514455, 0.43974006259672915, 0.4308599257517116, 0.4411006505638926, 0.4457247203768815, 0.4424944254417247, 0.4487223525397894, 0.451382347101148, 0.4494647591025669, 0.4765182665442764, 0.45395483355318467, 0.4787261443091646, 0.46735243063293597, 0.484702134134646, 0.4686154830082867, 0.48797026440483093, 0.49011005408677644, 0.4932690434793947, 0.5006557147805859, 0.494410995676112, 0.5060351812364063, 0.4949380416650535, 0.5074114644550844, 0.4979072523627903, 0.5151204474571203, 0.5048837441789988, 0.518684698894424, 0.5117049340490782, 0.5194313704895075, 0.5143273774842334, 0.523838098318415, 0.5225381808114893, 0.5391576226667638, 0.5328313942436966, 0.5415101547426029, 0.5421536140153655, 0.5439209583843079, 0.5537698575496424, 0.555899020387904, 0.5545705988558103, 0.5578417728539473, 0.5575076304892579, 0.5733091658299259, 0.5597002162622118, 0.5910155483090307, 0.563668311077075, 0.5922399487610637, 0.5755299083598109, 0.5981667731506417, 0.5757471420978457, 0.6041916521229456, 0.5840990778611321, 0.604199668470732, 0.5873079865235908, 0.6103041720973312, 0.6044907795503986, 0.6358448836048045, 0.6122467017422641, 0.6418057423542853, 0.6170709694231127, 0.6421874684173283, 0.6266097545778022, 0.6507354824339311, 0.6401626864500045, 0.6508581102334605, 0.6710184478919847, 0.6632053643068545, 0.6769824240153235, 0.6826219187914613, 0.6832837990188312, 0.6830598166706001, 0.6847222827594626, 0.6947365430889432, 0.6924565649903431, 0.6998270758634093, 0.6933528498608736, 0.7044080708181752, 0.6952368528251545, 0.7071380160351382, 0.6988685746355174, 0.707548384636, 0.7156600020652149, 0.725455401429584, 0.7169983315903496, 0.7333552742411844, 0.7200655403507232, 0.7357741353156961, 0.7373103847450087, 0.745480395218793, 0.7450898525998331, 0.7477948208902747, 0.7528698950064554, 0.7596236243253149, 0.7639747478741192, 0.7606912590451207, 0.7786107251206495, 0.7673892137837506, 0.7836121862980261, 0.775681279743603, 0.787479005597036, 0.7855141741202467, 0.7923055587064705, 0.8024036558519576, 0.8147691218889528, 0.8076916494640334, 0.8218558188075265, 0.8245881391897385, 0.8299622173323615, 0.8258860922555868, 0.8350532725131803, 0.8296368394903353, 0.8359115587091721, 0.8344502361340973, 0.836243081554398, 0.834578675985878, 0.8409813618958855, 0.8379729322196598, 0.8432689587016631, 0.8551044339671874, 0.8444864837971537, 0.858226107266868, 0.8575045399049008, 0.8610036936716048, 0.8626176594160395, 0.8699317725007567, 0.8662142901139547, 0.8707909740692155, 0.8697367875697521, 0.8726941100241117, 0.8718490765608018, 0.8749421564950342, 0.8780175064107547, 0.8826604432493645, 0.882800718339602, 0.8837731091504246, 0.8912191022865861, 0.8896592881880063, 0.8947560561182872, 0.8938598493885936, 0.9005754249786808, 0.9104908637933602, 0.9107293290546576, 0.9132398226702851, 0.9174356368248544, 0.9167158896870662, 0.9259662900925876, 0.9257383890008875, 0.9276114333587004, 0.9258497006605748, 0.9283073434729254, 0.9327581890628005, 0.9351509339969324, 0.9335848622389666, 0.9358319386497462, 0.9399724680869952, 0.9489620546888073, 0.9482965213556451, 0.959795664268857, 0.9605870095819469, 0.9611790673630538, 0.9645326679963915, 0.9693698619059363, 0.971225917917466, 0.9732152303283541, 0.9763826066801886, 0.9795021618269448, 0.9822139554679992, 0.982700500344724, 0.988665278574083, 0.993175629227931, 0.9908757949871649, 0.994542093289267]


* Question 9

/Critère d’arrêt : On décide d’arrêter lorsque la p-valeur dépasse 0.1. Illustrer la méthode en donnant:/

1. /les 3 graphes des statistiques obtenues lors de la sélection de la 1er, 2eme et 3eme variables (en abscisse : l’index des variables, en ordonné : la valeur des stats) ,/
2. /le graphe des 50 premières p-valeurs (dont chacune est associée à la variable sélectionnée). Sur ce même graphe, on tracera la ligne horizontale d’ordonnée 0.1. Enfin on donnera la liste des variables sélectionnées./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
result_trimmed = fast_forward_selection(X_train, Y_train, 0.1)
print("Variables selectionnees:", result_trimmed[0])
print("pvalues", result_trimmed[1])
#+END_SRC

#+RESULTS:
: Variables selectionnees: [58, 123, 167, 23, 34, 174, 99]
: pvalues [0.0, 4.594366220800339e-09, 1.9448257604359753e-07, 2.230973249894852e-05, 6.161408358718035e-05, 0.00014767475703725275, 0.00027147789041803705, 0.028673792423168587]

** Première variable

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
y = result_trimmed[2][0]
x = list(range(len(y)))

path9a = "./img/q9a.png"
plt.figure(0)
plt.scatter(x, y)
plt.title("T-stat pour selection 1ere variable")
plt.xlabel("Coefficient")
plt.ylabel("t-Stat")
plt.savefig(path9a)
plt.close()

path9a
#+END_SRC

#+RESULTS:
[[file:./img/q9a.png]]


** Seconde variable

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
y = result_trimmed[2][1]
x = list(range(len(y)))

path9b = "./img/q9b.png"
plt.figure(0)
plt.scatter(x, y)
plt.title("T-stat pour selection 2eme variable")
plt.xlabel("Coefficient")
plt.ylabel("t-Stat")
plt.savefig(path9b)
plt.close()

path9b
#+END_SRC

#+RESULTS:
[[file:./img/q9b.png]]

** Troisième variable

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
y = result_trimmed[2][2]
x = list(range(len(y)))

path9c = "./img/q9c.png"
plt.figure(0)
plt.scatter(x, y)
plt.title("T-stat pour selection 3eme variable")
plt.xlabel("Coefficient")
plt.ylabel("t-Stat")
plt.savefig(path9c)
plt.close()

path9c
#+END_SRC

#+RESULTS:
[[file:./img/q9c.png]]

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
y = result[1][:50]
y_limit = [0.1 for i in y]
x = range(len(y))
path9d = "./img/q9d.png"
plt.figure(0)
plt.scatter(x, y)
plt.scatter(x, y_limit)
plt.title("50 plus petites p-values")
plt.xlabel("Coefficient")
plt.ylabel("p-value")
plt.savefig(path9d)
plt.close()
path9d
#+END_SRC

#+RESULTS:
[[file:./img/q9d.png]]

* Question 10

/Appliquer OLS sur les variables sélectionnées. Donner le risque de prédiction obtenu l’échantillon test et le comparer à ceux de OLS et PCA before OLS./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
print("Fast forward variable selection (number of selected columns)", result_trimmed[0])
X_train_ffs = X_train[:,result_trimmed[0]]
model_ffs_ols = LinearRegression(fit_intercept=True).fit(X_train_ffs, Y_train)
Y_pred_ffs = model_ffs_ols.predict(X_test[:,result_trimmed[0]])
print("R2 for fast forward selection OLS", model_ffs_ols.score(X_test[:,result_trimmed[0]], Y_test))
R_pred_ffs = get_R_pred(Y_test, Y_pred_ffs)
print("R_pred for fast forward selection OLS:", R_pred_ffs)
print("R_pred_fastForward < R_pred_ols ?", R_pred_ffs < R_pred_ols)
print("R_pred_fastForward < R_pred_pca_ols ?", R_pred_ffs < R_pred_pca_ols)
#+END_SRC

#+RESULTS:
: Fast forward variable selection (number of selected columns) [58, 123, 167, 23, 34, 174, 99]
: R2 for fast forward selection OLS 0.4475310210603155
: R_pred for fast forward selection OLS: 2927.0644867704714
: R_pred_fastForward < R_pred_ols ? True
: R_pred_fastForward < R_pred_pca_ols ? True

Nous trouvons donc un risque de prédiction plus petit avec le modèle Ridge qu'avec OLS ou PCA before OLS.

* Question 11
  
/Afin de préparer la validation croisée, séparer l’échantillon train en 4 parties (appelées ”folds”) de façon aléatoire. On affichera les numéros d’échantillon sélectionnés dans chaque fold./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
kf = KFold(n_splits=4, random_state=10, shuffle=True)
_folds = kf.split(X_train)
folds = []

i = 0
for train_index, test_index in _folds:
  print("\n\nTRAIN", i, ":\n", train_index, "\nTEST", i, " :\n", test_index)
  folds.append([train_index, test_index])
  i += 1
#+END_SRC

#+RESULTS:
#+begin_example
TRAIN 0 :
 [  0   2   3   4   5   7   8   9  10  11  13  14  15  16  18  19  21  22
  23  28  30  31  32  33  35  36  37  38  39  40  41  42  44  45  46  48
  49  50  51  53  54  55  58  59  60  61  62  63  66  67  68  70  71  72
  73  74  75  77  79  81  82  83  84  85  86  89  90  91  93  94  95  96
  98  99 101 103 104 106 107 108 109 111 112 115 116 117 118 119 120 122
 123 124 125 127 128 129 131 132 133 134 135 136 137 140 141 143 144 145
 149 150 153 154 155 156 157 158 159 160 161 162 165 166 168 169 171 176
 177 178 179 180 182 184 185 186 187 188 190 191 192 193 194 195 196 198
 199 200 201 203 204 206 207 208 209 210 212 213 214 216 217 220 221 222
 223 224 226 228 230 232 233 234 235 236 237 238 239 240 241 242 243 244
 245 246 248 249 250 251 252 253 254 255 256 258 259 260 261 262 263 265
 267 268 269 270 271 273 274 275 276 279 281 283 284 285 286 287 288 289
 290 291 292 293 294 295 297 298 299 300 302 304 305 306 307 308 309 311
 312 313 315 316 318 319 320 321 322 323 324 326 327 328 329 330 331 332
 333 337 339 341 342 344 346 347 348 349 350 352] 
TEST 0  :
 [  1   6  12  17  20  24  25  26  27  29  34  43  47  52  56  57  64  65
  69  76  78  80  87  88  92  97 100 102 105 110 113 114 121 126 130 138
 139 142 146 147 148 151 152 163 164 167 170 172 173 174 175 181 183 189
 197 202 205 211 215 218 219 225 227 229 231 247 257 264 266 272 277 278
 280 282 296 301 303 310 314 317 325 334 335 336 338 340 343 345 351]


TRAIN 1 :
 [  1   2   3   4   6   7   8   9  11  12  13  14  15  16  17  18  20  22
  23  24  25  26  27  28  29  30  31  32  33  34  39  40  41  42  43  44
  45  47  48  51  52  53  54  56  57  62  64  65  66  67  69  71  72  73
  74  76  77  78  79  80  82  84  85  87  88  89  90  91  92  93  94  95
  96  97  99 100 101 102 103 105 107 109 110 112 113 114 115 116 117 118
 120 121 122 123 124 125 126 128 130 133 134 135 136 137 138 139 140 141
 142 143 144 145 146 147 148 149 150 151 152 153 155 156 158 159 161 162
 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180
 181 182 183 185 189 190 192 197 198 200 201 202 203 205 206 209 211 212
 213 214 215 216 218 219 220 221 224 225 226 227 228 229 231 232 233 234
 235 237 239 243 244 245 246 247 248 250 252 253 255 256 257 260 262 264
 265 266 268 271 272 273 274 275 277 278 279 280 281 282 283 284 286 287
 288 289 290 292 294 296 297 298 299 301 302 303 304 307 310 311 312 313
 314 315 316 317 318 319 320 321 323 325 326 327 328 329 330 331 334 335
 336 337 338 339 340 343 344 345 346 347 348 350 351] 
TEST 1  :
 [  0   5  10  19  21  35  36  37  38  46  49  50  55  58  59  60  61  63
  68  70  75  81  83  86  98 104 106 108 111 119 127 129 131 132 154 157
 160 184 186 187 188 191 193 194 195 196 199 204 207 208 210 217 222 223
 230 236 238 240 241 242 249 251 254 258 259 261 263 267 269 270 276 285
 291 293 295 300 305 306 308 309 322 324 332 333 341 342 349 352]


TRAIN 2 :
 [  0   1   3   4   5   6   8  10  11  12  13  14  15  16  17  18  19  20
  21  23  24  25  26  27  29  31  33  34  35  36  37  38  40  42  43  44
  45  46  47  49  50  51  52  54  55  56  57  58  59  60  61  62  63  64
  65  68  69  70  71  73  74  75  76  77  78  79  80  81  83  85  86  87
  88  89  92  93  94  97  98 100 102 104 105 106 108 110 111 112 113 114
 118 119 121 122 123 125 126 127 128 129 130 131 132 133 134 135 138 139
 140 141 142 143 145 146 147 148 150 151 152 154 156 157 158 160 163 164
 165 167 170 172 173 174 175 177 178 179 181 182 183 184 185 186 187 188
 189 191 193 194 195 196 197 199 200 201 202 203 204 205 207 208 210 211
 215 216 217 218 219 220 221 222 223 225 227 229 230 231 232 234 235 236
 237 238 239 240 241 242 243 246 247 248 249 250 251 254 256 257 258 259
 261 263 264 265 266 267 268 269 270 271 272 276 277 278 280 281 282 283
 284 285 286 289 290 291 292 293 295 296 300 301 303 305 306 307 308 309
 310 313 314 317 318 320 321 322 324 325 327 329 331 332 333 334 335 336
 337 338 339 340 341 342 343 345 347 349 350 351 352] 
TEST 2  :
 [  2   7   9  22  28  30  32  39  41  48  53  66  67  72  82  84  90  91
  95  96  99 101 103 107 109 115 116 117 120 124 136 137 144 149 153 155
 159 161 162 166 168 169 171 176 180 190 192 198 206 209 212 213 214 224
 226 228 233 244 245 252 253 255 260 262 273 274 275 279 287 288 294 297
 298 299 302 304 311 312 315 316 319 323 326 328 330 344 346 348]


TRAIN 3 :
 [  0   1   2   5   6   7   9  10  12  17  19  20  21  22  24  25  26  27
  28  29  30  32  34  35  36  37  38  39  41  43  46  47  48  49  50  52
  53  55  56  57  58  59  60  61  63  64  65  66  67  68  69  70  72  75
  76  78  80  81  82  83  84  86  87  88  90  91  92  95  96  97  98  99
 100 101 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 119
 120 121 124 126 127 129 130 131 132 136 137 138 139 142 144 146 147 148
 149 151 152 153 154 155 157 159 160 161 162 163 164 166 167 168 169 170
 171 172 173 174 175 176 180 181 183 184 186 187 188 189 190 191 192 193
 194 195 196 197 198 199 202 204 205 206 207 208 209 210 211 212 213 214
 215 217 218 219 222 223 224 225 226 227 228 229 230 231 233 236 238 240
 241 242 244 245 247 249 251 252 253 254 255 257 258 259 260 261 262 263
 264 266 267 269 270 272 273 274 275 276 277 278 279 280 282 285 287 288
 291 293 294 295 296 297 298 299 300 301 302 303 304 305 306 308 309 310
 311 312 314 315 316 317 319 322 323 324 325 326 328 330 332 333 334 335
 336 338 340 341 342 343 344 345 346 348 349 351 352] 
TEST 3  :
 [  3   4   8  11  13  14  15  16  18  23  31  33  40  42  44  45  51  54
  62  71  73  74  77  79  85  89  93  94 112 118 122 123 125 128 133 134
 135 140 141 143 145 150 156 158 165 177 178 179 182 185 200 201 203 216
 220 221 232 234 235 237 239 243 246 248 250 256 265 268 271 281 283 284
 286 289 290 292 307 313 318 320 321 327 329 331 337 339 347 350]
#+end_example

* Question 12

/Appliquer la méthode de la régression ridge. Pour le choix du paramètre de régularisation, on fera une validation croisée sur les ”folds” définies lors de la question précédente. A tour de rôle chacune des ”folds” servira pour calculer le risque de prédiction alors que les autres seront utilisées pour estimer le modèle. On moyennera ensuite les 4 risques de prédictions. On donnera la courbe du risque de validation croisée en fonction du paramètre de régularisation (on veillera à bien choisir l’espace de définition pour le graphe). Donner le paramètre de régularisation optimal et la valeur du risque sur le test./


#+BEGIN_SRC python :session default :exports both :results output :tangle yes
ridge_param_space = np.linspace(0, 100, 20)
def fit_ridge(X, Y, folds):
  R_pred_bars = []
  for alpha in ridge_param_space:
    R_preds = []
    for i_train, i_test in folds:
      clf = Ridge(alpha=alpha)
      clf.fit(X[i_train], Y[i_train])
      Y_pred = clf.predict(X[i_test])
      R_pred = get_R_pred(Y[i_test], Y_pred)
      R_preds.append(R_pred)

    R_pred_bar = np.mean(R_preds)
    R_pred_bars.append(R_pred_bar)
  return R_pred_bars

ridge_param_curve = fit_ridge(X_train, Y_train, folds)
print(ridge_param_curve)
#+END_SRC

#+RESULTS:
: [19725.057068212333, 3868.2429103202953, 3857.305752874005, 3847.401814139806, 3838.6896702631398, 3831.190810428525, 3824.84080763573, 3819.547978446685, 3815.215766700655, 3811.7513298983868, 3809.0687241961705, 3807.0898265696833, 3805.744275948606, 3804.968997146406, 3804.707567995887, 3804.9095525751727, 3805.529857829777, 3806.528138582898, 3807.8682597650263, 3809.517816662376]

Graphons maintenant notre courbe de paramètre Ridge:

#+BEGIN_SRC python :session default :exports both :results file :tangle yes
path11 = "./img/q11.png"
y = ridge_param_curve[:]
print(ridge_param_space)

ridge_param_space = list(np.linspace(0, 100, 20))
plt.figure(0)
plt.scatter(ridge_param_space[1:], y[1:])
plt.title("Ridge parameter curve")
plt.xlabel("Paramètre Ridge")
plt.ylabel("Risque de prediciton")
plt.savefig(path11)
plt.close()
path11
#+END_SRC

#+RESULTS:
[[file:./img/q11.png]]

Nous concluons donc que:

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
print("Le parametre Ridge optimal est:", ridge_param_space[np.argmin(ridge_param_curve)])
print("Et son risque est:", min(ridge_param_curve))
#+END_SRC

#+RESULTS:
: Le parametre Ridge optimal est: 73.6842105263158
: Et son risque est: 3804.707567995887

* Question 13
   
/A l’aide de la fonction lassoCV de sklearn, choisir le paramètre de régularisation du LASSO. Donner le risque de prédiction associé./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
lasso_model = LassoCV(cv=5)
lasso_model.fit(X_train, [y[0] for y in Y_train.tolist()])
Y_pred_lasso = lasso_model.predict(X_test)
R_pred_lasso = get_R_pred(Y_test, Y_pred_lasso)
print("Risque de prediction lasso: ", R_pred_lasso)
#+END_SRC

#+RESULTS:
: Risque de prediction lasso:  5389.933166498974

* Question 14
  
/Donner les variables selectionées par le lasso. Combien y-en a t-il ? Appliquer la méthode OLS aux variables sélectionnées. Cette méthode est appelé Least-square LASSO./

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
coefs = lasso_model.coef_
lasso_selected_cols = [i for i, c in enumerate(coefs) if c]
print("Le parametre Lasso optimal trouve est", lasso_model.alpha_)
print("Le lasso a selectionne les colonnes suivantes", lasso_selected_cols)
#+END_SRC

#+RESULTS:
: Le parametre Lasso optimal trouve est 6.631995961001097
: Le lasso a selectionne les colonnes suivantes [13, 58, 123, 129, 167, 174]

#+BEGIN_SRC python :session default :exports both :results output :tangle yes
model_ols_lasso = LinearRegression()
model_ols_lasso.fit(X_train[:,lasso_selected_cols], Y_train)
Y_pred_ols_lasso = model_ols_lasso.predict(X_test[:,lasso_selected_cols])
R_pred_ols_lasso = get_R_pred(Y_test, Y_pred_ols_lasso)
print("Le risque de prediciton de Least-square Lasso est ", R_pred_ols_lasso)
#+END_SRC

#+RESULTS:
: Le risque de prediciton de Least-square Lasso est  2806.624540445817

* Question 15

Cette dernière question est un question d’ouverture vers une approche non-linéaire. En utilisant les variables sélectionnées par le LASSO (Q13) ou par la méthode forward (Q9), mettre au point une méthode de régression non-linéaire. On apprendra les différents paramètres par validation croisée et l’on donnera la valeur du risque de prédiction calculé sur l’échantillon test. Des performances moindres par rapport à OLS peuvent se produire. Commenter.
