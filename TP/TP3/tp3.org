#+TITLE: Stats TP3
#+HTML_HEAD: <style>html { max-width: 50rem; margin: auto }</style>
#+HTML_HEAD: <style>.figure img { width: 100% }</style>
#+HTML_HEAD: <style>pre.example { background-color: aliceblue }</style>


#+begin_src bash :exports none
  mkdir -p img
  ls | grep img
#+end_src

* Introduction
  
Nous travaillons sur la base de données ~diabetes~ de python. La base initiale comporte $n = 442$ patients et $p = 10$ covariables. La variable $Y$ à expliquer est un score correspondant à l’évolution de la maladie. Pour s’amuser, un robot malicieux a contaminé le jeu de données en y ajoutant 200 variables explicatives inappropriées. Ensuite, non-content d’avoir déjà perverti notre jeu de données, il a volontairement mélangé les variables entre elle de façon aléatoire. Bien entendu le robot a ensuite pris soin d’effacer toute trace de son acte crapuleux si bien que nous ne connaissons pas les variables pertinentes. La nouvelle base de données comporte $n = 442$ patients et $p = 210$ covariables, notés $X$. Saurez-vous déjouer les plans de ce robot farceur et retrouver les variables pertinentes ?

* OLS Library
  
#+BEGIN_SRC python :session default :exports both :results output
def get_X(X_tild):
  X_tild = X_tild.tolist()
  X = [[1] +  X_tild[i] for i in range(0, len(X_tild))]
  return np.matrix(X)

# X [numpy matrix] is an n by p + 1 matrix
# Y [numpy matrix] is a column vector of n elements
# returns the n-vector for our estimator
def get_theta_hat(X, Y): 
    G = inv(X.T@X)
    theta_hat = G@X.T@Y
    
    return theta_hat

# Y [numpy matrix] - vector to calculate arithmetic mean against
# Returns the arithmetic mean of Y
def get_mean(Y):
    return sum(Y) / len(Y)

# x [numpy matrix] - the point to find an estimation for, a row vector of length n
# theta_hat [numpy matrix] - our estimator column vector of n elements
# Returns the estimation at point x given the estimator theta_hat
def get_Y_hat(X, theta_hat):
    return X@theta_hat

# Calculates the determination coefficient for our X matrix and Y vector
def get_R2(X_train, Y_train, X_test, Y_test):
    X_train = get_X(X_train)
    X_train = np.matrix(X_train)
    X_test = get_X(X_test)
    X_test = np.matrix(X_test)

    theta_hat = get_theta_hat(X_train, Y_train)
    Y_bar = get_Y_bar(Y_test)
    Y_hat = get_Y_hat(X_test, theta_hat)
    SSR = sum([(Y_hat[i] - Y_test[i])**2 for i in range(0, len(Y_test))])
    SST = sum([(Y_test[i] - Y_bar)**2 for i in range(0, len(Y_test))])
    return 1 - SSR / SST

# Noise variance for X and Y
def get_sig2(X, Y):
    theta_hat = get_theta_hat(X, Y)
    Y_hat = get_Y_hat(X, theta_hat)
    return sum([(Y_hat[i] - Y[i])**2 for i in range(0, len(Y))]) / (len(Y) - 2)

# Covariance matrix for X and Y
def get_estimator_covariance_mat(X, Y):
    sig2 = get_sig2(X, Y).item()
    return sig2 * inv(X.T@X)

# Helper pour calculer le rang d'une matrix X via Numpy
def rank(X):
    return np.linalg.matrix_rank(X)

# Calculates the t-test for the i'th estimator of our X-Y linear system
def student_test_i(X, Y, i):
    theta_hat = get_theta_hat(X, Y)
    var_thetaHat = get_estimator_covariance_mat(X, Y)
    return abs(theta_hat[i] / math.sqrt(var_thetaHat[i,i]))

# Returns a vector containing all the t-tests for our estimators (in the same order)
def student_test(X, Y):
    return [student_test_i(X, Y, i) for i in range(0, rank(X))]

# Helper to get Student distribution of degree deg an quantile of level lev
def get_students_quantile(deg, level):
    return scipy.stats.t(df=deg).ppf(level)

#+END_SRC

#+RESULTS:


* Question 1
  
/Importer la base de données ~data_dm3.csv~ disponible depuis le lien https://bitbucket.org/portierf/shared_files/downloads/data_dm3.csv. La dernière colonne est la variable à expliquer. Les autres colonnes sont les variables explicatives. Préciser le nombre de variables explicatives et le nombre d’observations./
  
#+BEGIN_SRC python :exports both :session default
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
import scipy.stats
from numpy.linalg import inv, eig
from scipy import stats
import statsmodels.api as sm
import random
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session default :exports both
def import_data():
    df = pd.read_csv('data_dm3.csv', delimiter=",", header=None)
    return df

df = import_data()
df.head()
#+END_SRC

#+RESULTS:
:         0         1         2         3         4         5         6    ...         204       205       206       207       208       209    210
: 0 -1.298173 -0.162249  1.223379  1.355554  1.080171  0.634979  0.298741  ...   -0.436399  0.279299 -1.416020 -2.332363  0.215096 -0.693319  151.0
: 1  0.166951 -0.338060 -0.618867  0.759366  1.134281 -0.536844 -0.075120  ...    1.119430  0.225958 -0.822288  0.382838 -0.718829 -0.188993   75.0
: 2 -0.416177 -0.205659 -1.282226  1.675500  1.523746  0.192029 -0.235840  ...   -2.579347  0.139267 -1.901196  0.048210  0.220205  0.471588  141.0
: 3  0.867184 -0.398667  0.093501  0.025971  1.852099  0.789774  0.801775  ...   -0.884172  0.723819  1.316367  0.088218  0.619496  1.061662  206.0
: 4  1.193282 -0.936980 -0.725039  0.766078  0.223489 -1.584622  1.146866  ...   -0.642504  2.040010 -1.703110 -1.901502  1.778811 -0.489853  135.0
: 
: [5 rows x 211 columns]


#+BEGIN_SRC python :session default :exports both
df.shape[1]
#+END_SRC

#+RESULTS:
: 211

#+BEGIN_SRC python :session default : exports both
num_df_cols = df.shape[1] - 1
dfX = df.drop(num_df_cols, axis=1)
dfX.head()
#+END_SRC

#+RESULTS:
:         0         1         2         3         4         5      ...          204       205       206       207       208       209
: 0 -1.298173 -0.162249  1.223379  1.355554  1.080171  0.634979    ...    -0.436399  0.279299 -1.416020 -2.332363  0.215096 -0.693319
: 1  0.166951 -0.338060 -0.618867  0.759366  1.134281 -0.536844    ...     1.119430  0.225958 -0.822288  0.382838 -0.718829 -0.188993
: 2 -0.416177 -0.205659 -1.282226  1.675500  1.523746  0.192029    ...    -2.579347  0.139267 -1.901196  0.048210  0.220205  0.471588
: 3  0.867184 -0.398667  0.093501  0.025971  1.852099  0.789774    ...    -0.884172  0.723819  1.316367  0.088218  0.619496  1.061662
: 4  1.193282 -0.936980 -0.725039  0.766078  0.223489 -1.584622    ...    -0.642504  2.040010 -1.703110 -1.901502  1.778811 -0.489853
: 
: [5 rows x 210 columns]

#+BEGIN_SRC python :session default :exports both
dfY = df[num_df_cols]
dfY.head()
#+END_SRC

#+RESULTS:
: 0    151.0
: 1     75.0
: 2    141.0
: 3    206.0
: 4    135.0
: Name: 210, dtype: float64

#+BEGIN_SRC python :session default :results output :exports both
print("Nombre de variable explicatives:", dfX.shape[1])
print("Numbre d'observations", dfX.shape[0])
#+END_SRC

#+RESULTS:
: Nombre de variable explicatives: 210
: Numbre d'observations 442

* Question 2
  
/Les variables explicatives sont-elles centrées ? Normalisées ? Qu’en est-il de la variable à expliquer ? Tracer un scatter plot de la base de données avec 4 covariables prises au hasard et la variable à expliquer (un scatterplot regroupe les graphes de chacune des variables en fonction de chacune des autres). Commenter les graphiques obtenus./

#+BEGIN_SRC python :session default :exports both
dfX.mean(axis=0)
#+END_SRC

#+RESULTS:
#+begin_example
0      7.535450e-19
1     -1.507090e-17
2      5.494599e-20
3     -7.284269e-18
4      8.288995e-18
5     -2.712762e-17
6      1.971776e-17
7      8.540177e-18
8      1.029845e-17
9      4.018907e-18
10    -1.444295e-17
11     3.717489e-17
12    -3.642134e-17
13    -1.124038e-17
14    -3.750456e-17
15    -4.511851e-17
16     2.461580e-17
17     9.293722e-18
18     2.662526e-17
19    -5.601351e-17
20    -3.067556e-17
21    -4.521270e-18
22     6.781905e-18
23    -4.056584e-17
24     1.004727e-18
25    -2.813235e-17
26    -3.540092e-17
27    -5.953006e-17
28    -4.533829e-17
29     3.064416e-17
           ...     
180   -3.767725e-18
181    3.843080e-17
182    4.018907e-18
183    6.380015e-17
184    1.795949e-17
185   -1.306145e-17
186    1.550053e-17
187    3.918434e-17
188    1.871304e-17
189    1.356381e-17
190   -2.737880e-17
191    2.210399e-17
192   -3.843080e-17
193    4.511851e-17
194   -6.530724e-18
195   -3.014180e-17
196    3.014180e-17
197   -2.888589e-17
198    7.887105e-17
199    3.918434e-17
200    1.934099e-17
201   -2.260635e-18
202   -2.637408e-17
203   -5.023634e-19
204   -1.538488e-17
205    5.525997e-18
206    3.265362e-17
207    1.507090e-17
208   -4.034606e-18
209    1.205672e-17
Length: 210, dtype: float64
#+end_example

#+BEGIN_SRC python :session default :exports both
dfX.var(axis=0)
#+END_SRC

#+RESULTS:
#+begin_example
0      1.002268
1      1.002268
2      1.002268
3      1.002268
4      1.002268
5      1.002268
6      1.002268
7      1.002268
8      1.002268
9      1.002268
10     1.002268
11     1.002268
12     1.002268
13     1.002268
14     1.002268
15     1.002268
16     1.002268
17     1.002268
18     1.002268
19     1.002268
20     1.002268
21     1.002268
22     1.002268
23     1.002268
24     1.002268
25     1.002268
26     1.002268
27     1.002268
28     1.002268
29     1.002268
         ...   
180    1.002268
181    1.002268
182    1.002268
183    1.002268
184    1.002268
185    1.002268
186    1.002268
187    1.002268
188    1.002268
189    1.002268
190    1.002268
191    1.002268
192    1.002268
193    1.002268
194    1.002268
195    1.002268
196    1.002268
197    1.002268
198    1.002268
199    1.002268
200    1.002268
201    1.002268
202    1.002268
203    1.002268
204    1.002268
205    1.002268
206    1.002268
207    1.002268
208    1.002268
209    1.002268
Length: 210, dtype: float64
#+end_example

#+BEGIN_SRC python :session default :exports both
dfY.mean(axis=0)
#+END_SRC

#+RESULTS:
: 152.13348416289594

#+BEGIN_SRC python :session default :exports both
dfY.var(axis=0)
#+END_SRC

#+RESULTS:
: 5943.331347923785


#+BEGIN_SRC python :session default :exports both :results file
def rand():
    return random.randint(0, dfX.shape[1] - 1)

rand_cols = [rand() for i in range(4)]

plt.figure(0)
sns_plot = sns.pairplot(dfX[rand_cols])
path2 = "./img/q2.png"
sns_plot.savefig(path2)
plt.close()
path2
#+END_SRC

#+RESULTS:
[[file:./img/q2.png]]


* Question 3
  
/Echantillon d’apprentissage et de test. Créer 2 échantillons : un pour apprendre le modèle $X_{\mbox{train}}$, un pour tester le modèle X test. On mettra 20% de la base dans l’échantillon ’test’. Donner les tailles de chacun des 2 échantillons. On notera que le nouvel échantillon de covariables $X_{\mbox{train}}$ n’est pas normalizé. Dans la suite, on fera donc bien attention à inclure l’intercept dans nos régression./

#+BEGIN_SRC python :session default :exports both :results output
X = np.matrix(dfX)
Y = np.matrix(dfY).T

X_test = X[0:88,:]
X_train = X[87:-1,:]

Y_test = Y[0:88]
Y_train = Y[87:-1]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("X test shape:", X_test.shape)
print("X train shape:", X_train.shape)
print("Y test shape:", Y_test.shape)
print("Y train shape:", Y_train.shape)
#+END_SRC

#+RESULTS:
: X test shape: (89, 210)
: X train shape: (353, 210)
: Y test shape: (89, 1)
: Y train shape: (353, 1)


* Question 4
  
/Donner la matrice de covariance calculée sur $X_{\mbox{train}}$. Tracer le graphe de la décroissance des valeurs propres de la matrice de corrélation. Expliquer pourquoi il est légitime de ne garder que les premières variables de l’ACP. On gardera 60 variables dans la suite./

La matrice des correlations est définie de la manière suivante:

$$Cor(X) = (X - \mathbb{E}(X))^T(X - \mathbb{E}(X))$$

Mais nous avons vu dans la question précedente que l'espérence de $X$ est nulle, donc notre matrice des correlations est égale à la matrice de Gram:

$$Cor(X) = X^TX$$

#+BEGIN_SRC python :session default :exports both :results file
u, s, vh = np.linalg.svd(X_train)

path3 = "./img/q3.png"
plt.figure(0)
plt.scatter(range(len(s)), s)
plt.savefig(path3)
plt.close()

path3
#+END_SRC

#+RESULTS:
[[file:./img/q3.png]]

* Question 5
  
/Suivant les observations de la question (Q4), appliquer la méthode de "PCA before OLS" qui consiste à appliquer OLS avec $Y$ et $X_{\mbox{train}}V_{1:60}$, où $V_{1:60}$ contient les vecteurs propres (associé aux 60 plus grandes valeurs propres) de la matrice de covariance. Faire une régression linéaire (avec intercept), puis tracer les valeurs des coefficients (hors intercept). Sur un autre graphique, faire de même avec la méthode des moindres carrés classique./


#+BEGIN_SRC python :session default :exports both :results output
u, s, vh = np.linalg.svd(X_train)
Xpca = X_train@vh.T[:,0:60]
print(Xpca.shape)
theta_hat_pca_before_ols = LinearRegression(fit_intercept=True).fit(Xpca, Y_train)
print(theta_hat_pca_before_ols.coef_)

#+END_SRC

#+RESULTS:
#+begin_example
(353, 60)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/tmp/babel-OQtQvi/python-9Bd97E", line 4, in <module>
    theta_hat_pca_before_ols = LinearRegression(fit_intercept=True).fit(Xpca, Y_train)
  File "/home/thomas/.local/lib/python3.7/site-packages/sklearn/linear_model/base.py", line 458, in fit
    y_numeric=True, multi_output=True)
  File "/home/thomas/.local/lib/python3.7/site-packages/sklearn/utils/validation.py", line 757, in check_X_y
    check_consistent_length(X, y)
  File "/home/thomas/.local/lib/python3.7/site-packages/sklearn/utils/validation.py", line 230, in check_consistent_length
    " samples: %r" % [int(l) for l in lengths])
ValueError: Found input variables with inconsistent numbers of samples: [353, 354]
#+end_example

#+BEGIN_SRC python :session default :exports both :results output
theta_hat_ols = LinearRegression(fit_intercept=True).fit(X_train, Y_train)
print(theta_hat_ols.intercept_)
print(X_train.shape)
#+END_SRC

#+RESULTS:
: [153.48863436]
: (354, 210)


#+BEGIN_SRC python :session default :exports both :results file
plt.figure(0)
path5 = "./q5b.png"
plt.scatter(range(theta_hat_pca_before_ols.coef_.shape[1]), theta_hat_pca_before_ols.coef_)
plt.savefig(path5)
plt.close()
path5
#+END_SRC

#+RESULTS:
[[file:./q5b.png]]


#+BEGIN_SRC python :session default :exports both :results file
plt.figure(0)
path4 = "./q5a.png"
plt.scatter(range(theta_hat_ols.coef_.shape[1]), theta_hat_ols.coef_)
plt.savefig(path4)
plt.close()
path4
#+END_SRC

#+RESULTS:
[[file:./q5a.png]]

* Question 6
  
/Donner les valeurs des intercepts pour les 2 régressions précédentes. Donner la valeur moyenne de la variable $Y$ (sur le train set). Les intercepts des 2 questions sont-ils égaux ? Commenter. Uniquement pour cette question, centrer et réduire les variables après ACP (de petite dimension). Faire une régression avec ces variables et vérifier que l’intercept est bien égal à la moyenne de $Y$ sur le train./

#+BEGIN_SRC python :session default :exports both :results output
print("Intercept OLS:", theta_hat_ols.intercept_)
print("Intercept PCA before OLS:", theta_hat_pca_before_ols.intercept_)
#+END_SRC

#+RESULTS:
: Intercept OLS: [153.48863436]
: Intercept PCA before OLS: [152.77106978]

#+BEGIN_SRC python :session default :exports both :results output
  Xpca_c = Xpca-Xpca.mean(axis=0)
  Xpca_n = Xpca_c/np.sqrt(Xpca.var(axis=0))

  q6_model = LinearRegression(fit_intercept=True).fit(Xpca_n, Y_train)
  print("Intercept for OLS after normalizationn of PCA:", q6_model.intercept_)
  print("Y_train mean:", Y_train.mean())
#+END_SRC

#+RESULTS:
: Intercept for OLS after normalizationn of PCA: [156.43785311]
: Y_train mean: 156.43785310734464

* Question 7

Pour les 2 méthodes (OLS et PCA before OLS) : Tracer les résidus de la prédiction sur l’échantillon test. Tracer leur densité (on pourra par exemple utiliser un histogramme). Calculer le coefficient de détermination sur l’échantillon test. Calculer le risque de prédiction sur l’échantillon test.

#+BEGIN_SRC python :session default :exports both :results output
Y_pred_pca = theta_hat_pca_before_ols.predict(X_test@vh.T[:,:60])
res_pca = Y_test - Y_pred_pca
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session default :exports both :results file
path7a = "./img/q7a.png"
plt.figure(0)
plt.scatter(range(len(res_pca)), [r[0] for r in res_pca])
plt.savefig(path7a)
plt.close()

path7a
#+END_SRC

#+RESULTS:
[[file:./img/q7a.png]]

#+BEGIN_SRC python :session default :exports both :results output
Y_pred_ols = theta_hat_ols.predict(X_test)
res_ols = Y_test - Y_pred_ols
#+END_SRC

#+RESULTS:

#+BEGIN_SRC python :session default :exports both :results file
path7b = "./img/q7b.png"
plt.figure(0)
plt.scatter(range(len(res_ols)), [r[0] for r in res_ols])
plt.savefig(path7b)
plt.close()

path7b
#+END_SRC

#+RESULTS:
[[file:./img/q7b.png]]

#+BEGIN_SRC python :session default :exports both :results file
f, axes = plt.subplots(1, 2)

plt.figure(0)
sns.set()
path7c = "./img/q7c.png"
plt7c = sns.distplot(res_pca, bins=20, kde=True, ax=axes[0])
plt7c = sns.distplot(res_ols, bins=20, kde=True, ax=axes[1])
axes[0].set_title("PCA before OLS")
axes[1].set_title("OLS")
plt7c.figure.savefig(path7c)

path7c
#+END_SRC

#+RESULTS:
[[file:./img/q7c.png]]

#+BEGIN_SRC python :session default :exports both :results output
print("R2 for PCA before OLS", theta_hat_pca_before_ols.score(X_test@vh.T[:,:60], Y_test))
print("R2 for OLS", theta_hat_ols.score(X_test, Y_test))

def get_R_pred(Y_test, Y_pred):
  A = Y_test - Y_pred
  return A.T@A / len(Y_test)

R_pred_pca_ols = get_R_pred(Y_test, Y_pred_pca)
R_pred_ols = get_R_pred(Y_test, Y_pred_ols)

print("R_pred PCA before OLS:", R_pred_pca_ols)
print("R_pred OLS:", R_pred_ols)
#+END_SRC

#+RESULTS:
: R2 for PCA before OLS 0.42767779780822845
: R2 for OLS 0.29118125887098567
: R_pred PCA before OLS: [[3032.24987626]]
: R_pred OLS: [[3755.42925269]]

* Question 8

Coder la méthode de forward variable sélection. On pourra utiliser la statistique du test de nullité du coefficient (comme vu en cours). Pour l’instant, on ne met pas de critère d’arret sur la méthode. C’est à dire que l’on ajoute une variable à chaque étape jusqu’à retrouver la totalité des variables. Afficher l’ordre de séléction des variables.

#+BEGIN_SRC python :session default :exports both :results output
def fast_forward_selection(X_train, Y_train):
  p = X_train.shape[1]
  n = X_train.shape[0]
  res_prev = Y_train
  X = get_X(X_train)
  tstats = []
  for i in range(p + 1):
    X_i = X[:, i]
    model_i = LinearRegression(fit_intercept=False).fit(X_i, res_prev)
    X_i_bar = get_mean(X_i)
    Y_pred = model_i.predict(X_i)
    res_new = Y_train - Y_pred
    sigma_est = np.sqrt((res_new.T@res_new)[0,0])
    tstat = model_i.coef_[0,0] * (1 / (n - 1) * sigma_est * np.sqrt(inv(X_i.T@X_i)[0,0]))
    tstats.append(tstat)
  return 0

fast_forward_selection(X_train, Y_train)
#+END_SRC

#+RESULTS:
